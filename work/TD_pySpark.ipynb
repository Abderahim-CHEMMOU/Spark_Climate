{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude de cas : analyse des fichiers de logs des cyclistes\n",
    "\n",
    "Objectif: A partir des fichiers contenu dans le dossier ./data/Cyclistes, calculer la durée de chacun des trajets effectués par chaque cycliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)  Charger la donnée\n",
    "Créez une seesion Spark et chargez les données Cyclistes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('timestamp', 'string'),\n",
       " ('sur_velo', 'string'),\n",
       " ('velo', 'string'),\n",
       " ('vitesse', 'string'),\n",
       " ('position', 'string'),\n",
       " ('destination_finale', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/Cyclistes\"\n",
    "df = spark.read.load(path, header=True,format=\"csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Vérifier le nombre de cyclistes\n",
    "\n",
    "Comptez le nombre d'id uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'ID uniques : 50\n"
     ]
    }
   ],
   "source": [
    "nombre_ids_uniques = df.select(\"id\").distinct().count()\n",
    "print(f\"Nombre d'ID uniques : {nombre_ids_uniques}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='12', timestamp='2018-01-01 00:01:00', sur_velo='False', velo='False', vitesse='0.030000000000000006', position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id='12', timestamp='2018-01-01 00:02:00', sur_velo='False', velo='False', vitesse='0.030000000000000006', position='(lon:2.07 lat:1.24)', destination_finale='False'),\n",
       " Row(id='12', timestamp='2018-01-01 00:03:00', sur_velo='False', velo='False', vitesse='0.030000000000000006', position='(lon:2.07 lat:1.24)', destination_finale='False')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Transformer la colonne timestamp\n",
    "\n",
    "Lorsqu'on vérifie le type de donnée de la colonne timestamp, on voit qu'on a une chaîne de caractères. Pour calculer une durée on voudrait transformer en date exploitable en tant que telle.\n",
    "A l'aide d'une fonction udf, créez une nouvelle colonne date qui contiendra le résultat de la transformation des chaînes de caractères de la colonne timestamp en véritables timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "\n",
    "@udf(returnType=TimestampType())\n",
    "def convertir_en_timestamp(valeur):\n",
    "    if valeur:\n",
    "        return datetime.datetime.strptime(valeur, \"%Y-%m-%d %H:%M:%S\") \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|          timestamp|               date|\n",
      "+-------------------+-------------------+\n",
      "|2018-01-01 00:01:00|2018-01-01 00:01:00|\n",
      "|2018-01-01 00:02:00|2018-01-01 00:02:00|\n",
      "|2018-01-01 00:03:00|2018-01-01 00:03:00|\n",
      "|2018-01-01 00:04:00|2018-01-01 00:04:00|\n",
      "|2018-01-01 00:05:00|2018-01-01 00:05:00|\n",
      "|2018-01-01 00:06:00|2018-01-01 00:06:00|\n",
      "|2018-01-01 00:07:00|2018-01-01 00:07:00|\n",
      "|2018-01-01 00:08:00|2018-01-01 00:08:00|\n",
      "|2018-01-01 00:09:00|2018-01-01 00:09:00|\n",
      "|2018-01-01 00:10:00|2018-01-01 00:10:00|\n",
      "|2018-01-01 00:11:00|2018-01-01 00:11:00|\n",
      "|2018-01-01 00:12:00|2018-01-01 00:12:00|\n",
      "|2018-01-01 00:13:00|2018-01-01 00:13:00|\n",
      "|2018-01-01 00:14:00|2018-01-01 00:14:00|\n",
      "|2018-01-01 00:15:00|2018-01-01 00:15:00|\n",
      "|2018-01-01 00:16:00|2018-01-01 00:16:00|\n",
      "|2018-01-01 00:17:00|2018-01-01 00:17:00|\n",
      "|2018-01-01 00:18:00|2018-01-01 00:18:00|\n",
      "|2018-01-01 00:19:00|2018-01-01 00:19:00|\n",
      "|2018-01-01 00:20:00|2018-01-01 00:20:00|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"date\", convertir_en_timestamp(df[\"timestamp\"]))\n",
    "df.select(\"timestamp\", \"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Durée des trajets par id.\n",
    "\n",
    "1) Trouvez les dates min/max par état de sur_velo, puis par id ET par état de sur_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|sur_velo|          min(date)|\n",
      "+--------+-------------------+\n",
      "|   False|2018-01-01 00:01:00|\n",
      "|    True|2018-01-01 01:47:00|\n",
      "+--------+-------------------+\n",
      "\n",
      "+--------+-------------------+\n",
      "|sur_velo|          max(date)|\n",
      "+--------+-------------------+\n",
      "|   False|2018-02-01 00:00:00|\n",
      "|    True|2018-01-31 21:32:00|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_par_sur_velo = df.groupBy(\"sur_velo\").agg(\n",
    "    {'date' : 'min'}\n",
    ")\n",
    "dates_par_sur_velo.show()\n",
    "\n",
    "dates_par_sur_velo = df.groupBy(\"sur_velo\").agg(\n",
    "    {'date' : 'max'}\n",
    ")\n",
    "dates_par_sur_velo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------------+\n",
      "| id|sur_velo|          min(date)|\n",
      "+---+--------+-------------------+\n",
      "| 12|   False|2018-01-01 00:01:00|\n",
      "| 29|   False|2018-01-01 00:01:00|\n",
      "| 43|    True|2018-01-01 07:44:00|\n",
      "| 36|    True|2018-01-01 08:47:00|\n",
      "| 41|    True|2018-01-01 08:38:00|\n",
      "| 43|   False|2018-01-01 00:01:00|\n",
      "| 36|   False|2018-01-01 00:01:00|\n",
      "| 41|   False|2018-01-01 00:01:00|\n",
      "| 29|    True|2018-01-01 05:24:00|\n",
      "| 15|   False|2018-01-01 00:01:00|\n",
      "| 12|    True|2018-01-01 08:57:00|\n",
      "| 15|    True|2018-01-01 10:19:00|\n",
      "| 14|   False|2018-01-01 00:01:00|\n",
      "| 42|   False|2018-01-01 00:01:00|\n",
      "| 23|   False|2018-01-01 00:01:00|\n",
      "| 47|    True|2018-01-01 04:09:00|\n",
      "| 47|   False|2018-01-01 00:01:00|\n",
      "| 10|    True|2018-01-01 08:35:00|\n",
      "| 14|    True|2018-01-01 07:48:00|\n",
      "| 42|    True|2018-01-01 05:57:00|\n",
      "+---+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------+-------------------+\n",
      "| id|sur_velo|          max(date)|\n",
      "+---+--------+-------------------+\n",
      "| 12|   False|2018-02-01 00:00:00|\n",
      "| 29|   False|2018-02-01 00:00:00|\n",
      "| 43|    True|2018-01-30 18:53:00|\n",
      "| 36|    True|2018-01-31 21:32:00|\n",
      "| 41|    True|2018-01-31 20:35:00|\n",
      "| 43|   False|2018-02-01 00:00:00|\n",
      "| 36|   False|2018-02-01 00:00:00|\n",
      "| 41|   False|2018-02-01 00:00:00|\n",
      "| 29|    True|2018-01-30 22:29:00|\n",
      "| 15|   False|2018-02-01 00:00:00|\n",
      "| 12|    True|2018-01-31 16:49:00|\n",
      "| 15|    True|2018-01-31 20:31:00|\n",
      "| 14|   False|2018-02-01 00:00:00|\n",
      "| 42|   False|2018-02-01 00:00:00|\n",
      "| 23|   False|2018-02-01 00:00:00|\n",
      "| 47|    True|2018-01-30 16:14:00|\n",
      "| 47|   False|2018-02-01 00:00:00|\n",
      "| 10|    True|2018-01-30 17:32:00|\n",
      "| 14|    True|2018-01-31 17:35:00|\n",
      "| 42|    True|2018-01-31 19:25:00|\n",
      "+---+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_par_sur_velo = df.groupBy(\"id\",\"sur_velo\").agg(\n",
    "    {'date' : 'min'}\n",
    ")\n",
    "dates_par_sur_velo.show()\n",
    "\n",
    "dates_par_sur_velo = df.groupBy(\"id\",\"sur_velo\").agg(\n",
    "    {'date' : 'max'}\n",
    ")\n",
    "dates_par_sur_velo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Le résultat n'est pas trés pertinent, il faudrait plutôt le début et la fin de chaque trajet par id. Pour cela, il faudrait détecter les changements d'état \"sur_vélo\".\n",
    "\n",
    "Créez une fonction python (voir fonction udf) qui permet de detecter ces changements d'état.\n",
    "Utilisez la classe Window() et la fonction F.lag() avec votre fonction udf pour créer une nouvelle colonne que vous appellerez changement, contenant un 0 si l'état précedent de sur_velo est le même et un 1 si l'état vient de changer pour chaque id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `changement` cannot be resolved. Did you mean one of the following? [`date`, `position`, `velo`, `vitesse`, `id`].;\n'Filter (('changement = 1) AND (cast(sur_velo#19 as boolean) = true))\n+- Project [id#17, timestamp#18, sur_velo#19, velo#20, vitesse#21, position#22, destination_finale#23, convertir_en_timestamp(timestamp#18)#47 AS date#48]\n   +- Relation [id#17,timestamp#18,sur_velo#19,velo#20,vitesse#21,position#22,destination_finale#23] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# window_spec = Window.partitionBy(\"id\").orderBy(\"timestamp\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# etat_precedent = lag(\"sur_velo\", 1).over(window_spec)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df = df.withColumn(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# sol 2\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m changement_a_true \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchangement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msur_velo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m changement_a_true\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msur_velo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchangement\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3325\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3323\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 3325\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3328\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3329\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3330\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `changement` cannot be resolved. Did you mean one of the following? [`date`, `position`, `velo`, `vitesse`, `id`].;\n'Filter (('changement = 1) AND (cast(sur_velo#19 as boolean) = true))\n+- Project [id#17, timestamp#18, sur_velo#19, velo#20, vitesse#21, position#22, destination_finale#23, convertir_en_timestamp(timestamp#18)#47 AS date#48]\n   +- Relation [id#17,timestamp#18,sur_velo#19,velo#20,vitesse#21,position#22,destination_finale#23] csv\n"
     ]
    }
   ],
   "source": [
    "# window_spec = Window.partitionBy(\"id\").orderBy(\"timestamp\")\n",
    "# etat_precedent = lag(\"sur_velo\", 1).over(window_spec)\n",
    "# df = df.withColumn(\n",
    "#     \"changement\",\n",
    "#     when(col(\"sur_velo\") != etat_precedent, 1).otherwise(0)\n",
    "# )\n",
    "# df.select(\"id\", \"timestamp\", \"sur_velo\", \"changement\").show(500)\n",
    "\n",
    "#solution prof\n",
    "# df.select(\"id\", \"timestamp\", \"sur_velo\", \"changement\").sort(\"id\", \"timestamp\").filter((df.sur_velo == True)).show(10)\n",
    "\n",
    "# sol 2\n",
    "changement_a_true = df.filter((col(\"changement\") == 1) & (col(\"sur_velo\") == True))\n",
    "changement_a_true.select(\"id\", \"timestamp\", \"sur_velo\", \"changement\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Grâce à cette nouvelle colonne changement, trouvez un moyen qui permettra de numeroter les trajets pour chaque id et stockez les résulats dans une nouvelle colonne appelée numero_de_trajet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"timestamp\")\n",
    "df = df.withColumn(\n",
    "    \"numero_de_trajet\",\n",
    "    sum(when(col(\"changement\") == 1, 1).otherwise(0)).over(window_spec)\n",
    ")\n",
    "df.select(\"id\", \"timestamp\", \"sur_velo\", \"changement\", \"numero_de_trajet\").show(550)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Il suffit maintenant de repêter la première étape, c'est a dire récupérer la début et la fin de chaque trajet pour chaque id, puis calculer la durée des trajets. \n",
    "\n",
    "(Pensez à récuperer les \"vrai trajet au préalable (avec un état sur_vélo = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer uniquement les lignes où sur_velo est True\n",
    "vrais_trajets = df.filter(col(\"sur_velo\") == True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Data visualisation\n",
    "\n",
    "Convertissez votre dataframe pyspark en dataframe pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) A l'aide des librairies matplotlib et/ou seaborn, réalisez un graphique en barre montrant le temps total passé à vélo par chaque cycliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Réalisez un graphique en barre qui affiche le temps de chaque trajet d'un cycliste. Faites en sorte qu'on puisse choisir un id et afficher les trajets de cet id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Sauvegarde\n",
    "\n",
    "Sauvegardez votre dataset trajets au format csv dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
